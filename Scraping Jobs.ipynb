{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77772446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cb1612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_description(job_url):\n",
    "    \"\"\"\n",
    "    This function scrapes the job desciption and requirements.\n",
    "    Args: url of the job web page\n",
    "    Returns: job description\n",
    "    \"\"\"\n",
    "    job_response = requests.get(job_url)\n",
    "    # Check if the web page exists\n",
    "    if job_response.status_code == 200:\n",
    "        job_src = job_response.text\n",
    "        job_soup = BeautifulSoup(job_src, 'lxml')\n",
    "        \n",
    "        # Find job description\n",
    "        desc_div = job_soup.find('h2', string='Job Description').find_next('div', class_='t-break')\n",
    "        job_desc = ''\n",
    "        if desc_div:\n",
    "            desc = [item.text.strip() for item in desc_div]\n",
    "            job_desc = ' '.join(desc)\n",
    "        \n",
    "        # Find skills\n",
    "        skills_divs = job_soup.find_all('div', class_='card-content')\n",
    "        skills_desc = ''\n",
    "        for card in skills_divs:\n",
    "            if card.find_next('h2', string='Skills'):\n",
    "                skills_items = card.find_all(['p', 'li'])\n",
    "                if skills_items:\n",
    "                    skills = [item.text.strip() for item in skills_items]\n",
    "                    skills_desc = ' '.join(skills)\n",
    "        \n",
    "        # Add skills to job_desc if job_desc already has content\n",
    "        if job_desc:\n",
    "            job_desc += ' ' + skills_desc\n",
    "        else:\n",
    "            job_desc = skills_desc\n",
    "        \n",
    "        # Parse job description and skills\n",
    "        desc_soup = BeautifulSoup(job_desc, 'lxml')\n",
    "        return desc_soup\n",
    "    \n",
    "    else:\n",
    "        print('HTTP request failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d750052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_job(search):\n",
    "    \"\"\"\n",
    "    This function iterates over every page of a job search and extracts info from each job posting.\n",
    "    Args: job search\n",
    "    Returns: a nested list with each sublist containing info about a job posting\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    pages_num = num_of_pages(search)\n",
    "    search = search.replace(' ', '-')\n",
    "    base_url = f'https://www.bayt.com/en/egypt/jobs/{search}-jobs'\n",
    "    base_response = requests.get(base_url)\n",
    "    if base_response.status_code == 200:\n",
    "        base_src = base_response.text\n",
    "        base_soup = BeautifulSoup(base_src, 'lxml')\n",
    "        # Get the total number of jobs and divide it by the maximum number of jobs listed in each page and round up\n",
    "        jobs_num = base_soup.find('div', class_='col p0').text[1:-12]\n",
    "        pages_num = math.ceil(int(jobs_num) / 20)\n",
    "    else:\n",
    "        print('HTTP request failed.')\n",
    "    \n",
    "    for page_num in range(1, pages_num+1):\n",
    "        page_url = f'https://www.bayt.com/en/egypt/jobs/{search}-jobs/?page={page_num}'\n",
    "        page_response = requests.get(page_url)\n",
    "        if page_response.status_code == 200:\n",
    "            page_src = page_response.text\n",
    "            page_soup = BeautifulSoup(page_src, 'lxml')\n",
    "            cards = page_soup.find_all('li', class_='has-pointer-d')\n",
    "            for card in cards:\n",
    "                row = []\n",
    "                job_path = card.find('a', href=True)['href']\n",
    "                job_url = f'https://www.bayt.com{job_path}'\n",
    "                job_title = card.find('a').text[1:-1]\n",
    "                company = card.find('b', class_='jb-company').text\n",
    "                location = card.find('span', class_='jb-loc').text\n",
    "                desc_soup = scrape_description(job_url)\n",
    "                if desc_soup.find('p'):  # Get the text of each paragraph in  the soup\n",
    "                    for paragraph in desc_soup.find('p'):\n",
    "                        job_desc = paragraph.text\n",
    "                # Add the extracted info to row (each job)\n",
    "                row.append(job_url)\n",
    "                row.append(job_title)\n",
    "                row.append(company)\n",
    "                row.append(location)\n",
    "                row.append(job_desc)\n",
    "                # Then add row to rows (all jobs)\n",
    "                rows.append(row)\n",
    "        else:\n",
    "            print('HTTP request failed.')\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded47e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['job_url', 'job_title', 'company', 'location', 'job_desc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb82d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_search = 'data analysis'\n",
    "da_rows = scrape_job(da_search)\n",
    "da_df = pd.DataFrame(da_rows, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780f2484",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_search = 'data science'\n",
    "ds_rows = scrape_job(ds_search)\n",
    "ds_df = pd.DataFrame(ds_rows, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba5c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_search = 'data engineer'\n",
    "de_rows = scrape_job(de_search)\n",
    "de_df = pd.DataFrame(de_rows, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7707af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_search = 'machine learning'\n",
    "ml_rows = scrape_job(ml_search)\n",
    "ml_df = pd.DataFrame(ml_rows, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [de_df, ds_df, da_df, ml_df]\n",
    "jobs_df = pd.concat(data, axis=0)\n",
    "jobs_df.to_csv('jobs_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a647d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f872c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
